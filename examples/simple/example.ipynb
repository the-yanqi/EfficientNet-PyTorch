{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In this simple example, we load an image, pre-process it, and classify it with a pretrained EfficientNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'efficientnet-b0'\n",
    "image_size = EfficientNet.get_image_size(model_name) # 224\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open image\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "file_name = '/Users/yanqixu/Documents/1.0.MasterCDS/project-summer/breast_img/0_L-CC.png'\n",
    "loaded_image = np.array(imageio.imread(file_name)).astype(np.float32)\n",
    "\n",
    "loaded_image = np.expand_dims(np.expand_dims(loaded_image, 0), 0).copy()\n",
    "mammo = torch.Tensor(loaded_image)\n",
    "# mammo = torch.cat([tensor_batch,tensor_batch],0)\n",
    "# mammo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert python 2D array into 4D torch tensor in N,C,H,W format\n",
    "loaded_image = np.expand_dims(np.expand_dims(loaded_image, 0), 0).copy()\n",
    "tensor_batch = torch.Tensor(loaded_image)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "img = Image.open('img.jpg')\n",
    "tfms = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), \n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "img = tfms(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_new = torch.cat([torch.cat([img[:,:,:,:],torch.zeros(1,3,224,256-224)],3),torch.zeros(1,3,256-224,256)],2)\n",
    "img_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class names\n",
    "labels_map = json.load(open('labels_map.txt'))\n",
    "labels_map = [labels_map[str(i)] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "-----\n",
      "window screen                                                               (33.92%)\n",
      "fire screen, fireguard                                                      (18.37%)\n",
      "hamper                                                                      (1.62%)\n",
      "envelope                                                                    (1.31%)\n",
      "radiator                                                                    (0.88%)\n"
     ]
    }
   ],
   "source": [
    "# Classify with EfficientNet\n",
    "model = EfficientNet.from_pretrained(model_name,in_channels=1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(img_new)\n",
    "preds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n",
    "\n",
    "print('-----')\n",
    "for idx in preds:\n",
    "    label = labels_map[idx]\n",
    "    prob = torch.softmax(logits, dim=1)[0, idx].item()\n",
    "    print('{:<75} ({:.2f}%)'.format(label, prob*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = EfficientNet.from_name(model_name,in_channels=1,stem_filters=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem torch.Size([1, 16, 1472, 960])\n",
      "In MBConvBlock\n",
      "torch.Size([1, 16, 737, 481])\n",
      "torch.Size([1, 24, 368, 240])\n",
      "torch.Size([1, 24, 368, 240])\n",
      "torch.Size([1, 40, 184, 120])\n",
      "torch.Size([1, 40, 184, 120])\n",
      "torch.Size([1, 80, 92, 60])\n",
      "torch.Size([1, 80, 92, 60])\n",
      "torch.Size([1, 80, 92, 60])\n",
      "torch.Size([1, 112, 92, 60])\n",
      "torch.Size([1, 112, 92, 60])\n",
      "torch.Size([1, 112, 92, 60])\n",
      "torch.Size([1, 192, 46, 30])\n",
      "torch.Size([1, 192, 46, 30])\n",
      "torch.Size([1, 192, 46, 30])\n",
      "torch.Size([1, 192, 46, 30])\n",
      "torch.Size([1, 320, 46, 30])\n"
     ]
    }
   ],
   "source": [
    "output = new_model.extract_features(mammo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "torch.Size([1, 32, 128, 128])\n",
      "In MBConvBlock\n",
      "torch.Size([1, 16, 128, 128])\n",
      "torch.Size([1, 24, 64, 64])\n",
      "torch.Size([1, 24, 64, 64])\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 40, 32, 32])\n",
      "torch.Size([1, 80, 16, 16])\n",
      "torch.Size([1, 80, 16, 16])\n",
      "torch.Size([1, 80, 16, 16])\n",
      "torch.Size([1, 112, 16, 16])\n",
      "torch.Size([1, 112, 16, 16])\n",
      "torch.Size([1, 112, 16, 16])\n",
      "torch.Size([1, 192, 8, 8])\n",
      "torch.Size([1, 192, 8, 8])\n",
      "torch.Size([1, 192, 8, 8])\n",
      "torch.Size([1, 192, 8, 8])\n",
      "torch.Size([1, 320, 8, 8])\n",
      "torch.Size([1, 1280, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "local_model = EfficientNet.from_name(model_name,in_channels=3)\n",
    "output2 = local_model.forward(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25',\n",
    "        'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25',\n",
    "        'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25',\n",
    "        'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "blocks_args = BlockDecoder.decode(blocks_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockArgs(num_repeat=6, kernel_size=3, stride=[1], expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, id_skip=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_args[0]._replace(num_repeat=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=2, kernel_size=3, stride=[2], expand_ratio=6, input_filters=16, output_filters=24, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=2, kernel_size=5, stride=[2], expand_ratio=6, input_filters=24, output_filters=40, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=3, kernel_size=3, stride=[2], expand_ratio=6, input_filters=40, output_filters=80, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=3, kernel_size=5, stride=[1], expand_ratio=6, input_filters=80, output_filters=112, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=4, kernel_size=5, stride=[2], expand_ratio=6, input_filters=112, output_filters=192, se_ratio=0.25, id_skip=True),\n",
       " BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=6, input_filters=192, output_filters=320, se_ratio=0.25, id_skip=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = GlobalParams(\n",
    "        width_coefficient=1,\n",
    "        depth_coefficient=1,\n",
    "        image_size=1,\n",
    "        dropout_rate=1,\n",
    "\n",
    "        num_classes=2,\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        drop_connect_rate=1,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "        stem_filters = 32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalParams(width_coefficient=1, depth_coefficient=1, image_size=1, dropout_rate=1, num_classes=10, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=1, depth_divisor=8, min_depth=None, stem_filters=32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_params._replace(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalParams(width_coefficient=1, depth_coefficient=1, image_size=1, dropout_rate=1, num_classes=2, batch_norm_momentum=0.99, batch_norm_epsilon=0.001, drop_connect_rate=1, depth_divisor=8, min_depth=None, stem_filters=32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
